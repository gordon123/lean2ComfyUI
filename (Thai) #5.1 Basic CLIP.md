# Stable diffusion ‡∏Ñ‡∏∑‡∏≠‡∏≠‡∏∞‡πÑ‡∏£

![alt text](image-4.png)
 ‡∏ñ‡πâ‡∏≤‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏©‡∏≤‡∏ö‡πâ‡∏≤‡∏ô ‡πÜ ‡∏î‡∏π‡∏ï‡∏≤‡∏°‡∏£‡∏π‡∏õ‡πÄ‡∏•‡∏¢ ‡∏Ñ‡∏∑‡∏≠ ‡πÄ‡∏Ç‡∏µ‡∏¢‡∏ô‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏•‡πâ‡∏ß‡πÉ‡∏ä‡πâ‡πÇ‡∏°‡πÄ‡∏î‡∏• ‡∏Ç‡∏≠‡∏á Stable diffusion  ‡∏´‡∏•‡∏±‡∏Å ‡πÜ ‡∏™‡∏≤‡∏°‡∏ï‡∏±‡∏ß‡∏ô‡∏µ‡πâ ‡∏Ñ‡∏∑‡∏≠ CLIP, U-NET ‡πÅ‡∏•‡∏∞ VAE
 ‡∏à‡∏ö ‡∏Å‡∏≤‡∏£‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢ ‡πÅ‡∏ö‡∏ö‡∏á‡πà‡∏≤‡∏¢ ‡πÜ ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡πÅ‡∏Ñ‡πà‡∏ô‡∏µ‡πâ ‡πÄ‡∏à‡∏ô‡∏£‡∏π‡∏õ‡πÑ‡∏î‡πâ‡πÅ‡∏•‡πâ‡∏ß

---
 ‡πÅ‡∏ï‡πà‡∏ñ‡πâ‡∏≤‡πÉ‡∏Ñ‡∏£‡∏≠‡∏¢‡∏≤‡∏Å‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏°‡∏≤‡∏Å‡∏Ç‡∏∂‡πâ‡∏ô (‡∏õ‡∏ß‡∏î‡∏´‡∏±‡∏ß‡∏°‡∏≤‡∏Å‡∏Å‡∏ß‡πà‡∏≤‡∏ô‡∏µ‡πâ) ‡∏•‡∏≠‡∏á‡∏ï‡∏≤‡∏°‡∏≠‡πà‡∏≤‡∏ô‡∏ï‡πà‡∏≠

‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ó‡∏µ‡πà‡∏ö‡∏≠‡∏Å Stable diffusion ‡∏°‡∏µ‡∏≠‡∏á‡∏Ñ‡πå‡∏õ‡∏£‡∏∞‡∏Å‡∏≠‡∏ö‡∏´‡∏•‡∏±‡∏Å ‡πÜ  3 ‡πÇ‡∏°‡πÄ‡∏î‡∏• 

1. CLIP - ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô‡∏ï‡∏±‡∏ß‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà ‡∏†‡∏≤‡∏û‡∏Å‡∏•‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡πÄ‡∏Ñ‡∏£‡∏∑‡πà‡∏≠‡∏á‡∏°‡∏∑‡∏≠ ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ ‡∏Å‡∏≤‡∏£ Transformer (‡πÅ‡∏õ‡∏•‡∏á‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç ‡πÉ‡∏ô‡∏£‡∏π‡∏õ Vecter, matric, Tensors, Flat number) ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤ Vision Transformer (ViT) ‡∏ñ‡πâ‡∏≤‡πÄ‡∏õ‡πá‡∏ô ‡∏ï‡∏±‡∏ß‡∏≠‡∏±‡∏Å‡∏©‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡∏ß‡πà‡∏≤  Text transformer
![alt text](image-8.png)
2. U-NET ‡∏Ñ‡∏∑‡∏≠‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏†‡∏≤‡∏û‡∏î‡πâ‡∏ß‡∏¢ Noise ‡∏£‡∏≠‡∏≠‡πà‡∏≤‡∏ô ‡∏Ç‡πâ‡∏≠ 5.2
3. VAE - ‡∏ï‡∏±‡∏ß‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡∏´‡∏±‡∏™ ‡πÄ‡∏£‡∏µ‡∏¢‡∏Å‡πÄ‡∏ó‡πà ‡πÜ ‡∏ß‡πà‡∏≤ Encode ‡∏à‡∏≤‡∏Å Pixel (‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û 512 x 512 x 3 RGB color ) ‡πÄ‡∏õ‡πá‡∏ô Latent (64 x 64 x 4 channels) ‡πÅ‡∏•‡∏∞‡∏ï‡∏±‡∏ß‡∏ñ‡∏≠‡∏î‡∏£‡∏´‡∏±‡∏™ ‡πÅ‡∏õ‡∏•‡∏á Latent ‡∏°‡∏≤‡πÄ‡∏õ‡πá‡∏ô Pixel ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û ‡∏£‡∏≠‡∏≠‡πà‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠ 5.3


## 5.1 ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á CLIP
‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á CLIP ‡∏™‡∏°‡∏°‡∏∏‡∏ï‡∏¥‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡πÄ‡∏ò‡∏≠" <br><br>
**CLIP (Contrastive Language-Image Pretraining)** ‡πÄ‡∏õ‡πá‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏î‡∏¢ OpenAI ‡∏ã‡∏∂‡πà‡∏á‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° (Text) ‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û (Image) ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Contrastive Learning ‡πÄ‡∏°‡∏∑‡πà‡∏≠ CLIP ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÄ‡∏ä‡πà‡∏ô "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡πÄ‡∏ò‡∏≠", ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡∏∏‡∏õ‡πÑ‡∏î‡πâ‡πÄ‡∏õ‡πá‡∏ô 4 ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡∏´‡∏•‡∏±‡∏Å: ‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ<br>

1Ô∏è‚É£ Tokenization (‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç‡∏ó‡∏µ‡πà‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à)

üîπ ‡∏Ç‡∏±‡πâ‡∏ô‡∏ï‡∏≠‡∏ô‡πÅ‡∏£‡∏Å, ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡πÄ‡∏ò‡∏≠" ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡∏™‡πà‡∏á‡πÑ‡∏õ‡∏¢‡∏±‡∏á Text Encoder <br>
üîπ CLIP ‡πÉ‡∏ä‡πâ Byte-Pair Encoding (BPE) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô Token IDs ‡πÄ‡∏ä‡πà‡∏ô [[1, 1, 1, 1]]<br>

2Ô∏è‚É£ Text Encoding (‡πÅ‡∏õ‡∏•‡∏á Token ‡πÄ‡∏õ‡πá‡∏ô Vector ‡∏ù‡∏±‡∏á‡∏ï‡∏±‡∏ß - Embedding) <br>
‡∏´‡∏•‡∏±‡∏á‡∏à‡∏≤‡∏Å Tokenization, CLIP ‡∏à‡∏∞‡πÉ‡∏ä‡πâ Transformer-based Text Encoder ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå (Text Embedding)

üí° ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡πÉ‡∏ô Text Encoder

‡πÉ‡∏ä‡πâ Pre-trained Transformer Model ‡πÄ‡∏ä‡πà‡∏ô ViT ‡∏´‡∏£‡∏∑‡∏≠ ResNet
‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡πÄ‡∏ò‡∏≠" ‡πÄ‡∏õ‡πá‡∏ô Vector Embedding ‡∏Ç‡∏ô‡∏≤‡∏î 512 ‡∏´‡∏£‡∏∑‡∏≠ 768 ‡∏°‡∏¥‡∏ï‡∏¥
Self-Attention Mechanism ‡∏ä‡πà‡∏ß‡∏¢‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏≥‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÉ‡∏ô‡∏ö‡∏£‡∏¥‡∏ö‡∏ó
 ### ‡πÄ‡∏ä‡πà‡∏ô Output: torch.Size([1, 4, 768])

| ‡πÇ‡∏°‡πÄ‡∏î‡∏•        | ‡∏´‡∏ô‡πâ‡∏≤‡∏ó‡∏µ‡πà‡∏´‡∏•‡∏±‡∏Å                       | ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Stable Diffusion ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÑ‡∏£?  | Hugging Face Model | Research Paper |
|-------------|--------------------------------|--------------------------------|--------------------|----------------|
| **CLIP**    | Text-to-Image Matching        | ‡πÅ‡∏õ‡∏•‡∏á Prompt ‡πÄ‡∏õ‡πá‡∏ô Text Embedding | [openai/clip-vit-large-patch14](https://huggingface.co/openai/clip-vit-large-patch14) | [CLIP: Contrastive Language-Image Pretraining](https://arxiv.org/abs/2103.00020) |
| **T5**      | Text-to-Text Learning         | ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô Prompt Engineering | [t5-base](https://huggingface.co/t5-base) | [Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer](https://arxiv.org/abs/1910.10683) |
| **BART**    | Denoising & Summarization     | ‡πÉ‡∏ä‡πâ‡∏õ‡∏£‡∏±‡∏ö‡∏õ‡∏£‡∏∏‡∏á Prompt ‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ | [facebook/bart-large](https://huggingface.co/facebook/bart-large) | [BART: Denoising Sequence-to-Sequence Pre-training](https://arxiv.org/abs/1910.13461) |
| **ViT**     | Image Understanding           | ‡πÉ‡∏ä‡πâ‡∏ï‡∏£‡∏ß‡∏à‡∏™‡∏≠‡∏ö‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏à‡∏≤‡∏Å SD | [google/vit-base-patch16-224](https://huggingface.co/google/vit-base-patch16-224) | [An Image is Worth 16x16 Words: Transformers for Image Recognition](https://arxiv.org/abs/2010.11929) |
| **LDM**     | Latent Diffusion Model        | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏´‡∏•‡∏±‡∏Å‡∏Ç‡∏≠‡∏á Stable Diffusion | [CompVis/stable-diffusion-v1-4](https://huggingface.co/CompVis/stable-diffusion-v1-4) | [High-Resolution Image Synthesis with Latent Diffusion Models](https://arxiv.org/abs/2112.10752) |

3Ô∏è‚É£ ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡∏Å‡∏±‡∏ö Image Embeddings (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö) <br>
‡∏´‡∏≤‡∏Å CLIP ‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏Ñ‡∏π‡πà‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡πÄ‡∏ò‡∏≠", ‡∏£‡∏∞‡∏ö‡∏ö‡∏à‡∏∞‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏î‡∏±‡∏á‡∏ô‡∏µ‡πâ:

üîπ Image Encoding (‡πÄ‡∏õ‡∏•‡∏µ‡πà‡∏¢‡∏ô‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå)

CLIP ‡πÉ‡∏ä‡πâ Vision Transformer (ViT) ‡∏´‡∏£‡∏∑‡∏≠ ResNet
‡πÅ‡∏õ‡∏•‡∏á‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡πá‡∏ô Image Embedding ‡∏Ç‡∏ô‡∏≤‡∏î 768 ‡∏°‡∏¥‡∏ï‡∏¥ <br>
üîπ ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ñ‡∏•‡∏∂‡∏á‡∏Å‡∏±‡∏ô ***(Cosine Similarity)***

‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Cosine Similarity ‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á Text Embedding ‡πÅ‡∏•‡∏∞ Image Embedding
‡∏´‡∏≤‡∏Å ‡∏Ñ‡πà‡∏≤‡∏™‡∏π‡∏á ‡πÅ‡∏™‡∏î‡∏á‡∏ß‡πà‡∏≤ ‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡∏ï‡∏£‡∏á‡∏Å‡∏±‡∏ô

4Ô∏è‚É£ Output & Decision (‡∏ï‡∏±‡∏î‡∏™‡∏¥‡∏ô‡πÉ‡∏à‡∏ú‡∏•‡∏•‡∏±‡∏û‡∏ò‡πå) <br>
üîπ ‡∏´‡∏≤‡∏Å Cosine Similarity ‡∏™‡∏π‡∏á, ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏≠‡∏≤‡∏à ‡πÅ‡∏™‡∏î‡∏á‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡πÄ‡∏ò‡∏≠"<br>
üîπ ‡∏´‡∏≤‡∏Å‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏û‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö, CLIP ‡∏≠‡∏≤‡∏à‡πÉ‡∏ä‡πâ embedding ‡πÄ‡∏û‡∏∑‡πà‡∏≠:<br>

‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÉ‡∏Å‡∏•‡πâ‡πÄ‡∏Ñ‡∏µ‡∏¢‡∏á‡∏ó‡∏µ‡πà‡∏™‡∏∏‡∏î ‡πÉ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•
‡∏û‡∏¢‡∏≤‡∏Å‡∏£‡∏ì‡πå‡∏ß‡πà‡∏≤‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏≠‡∏∞‡πÑ‡∏£ <br>

# üñºÔ∏è Step-by-Step ‡∏Å‡∏≤‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Embedding Calculation

## **1Ô∏è‚É£ ‡πÅ‡∏õ‡∏•‡∏á ‡∏£‡∏π‡∏õ ‡∏´‡∏£‡∏∑‡∏≠ Text ‡πÄ‡∏õ‡πá‡∏ô vector (4D)**
| **Input**   | **Vector Representation** |
|------------|--------------------------|
| üê± **Image of a Cat**  | `[0.2, 0.7, 0.1, 0.9]` |
| **"A cat"**  | `[0.2, 0.7, 0.1, 0.9]` |
| **"A dog"**  | `[0.3, 0.6, 0.2, 0.8]` |
| **"A car"**  | `[0.9, 0.1, 0.4, 0.2]` |

---

## 2Ô∏è‚É£ ‡∏™‡∏π‡∏ï‡∏£‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Cosine similarity
\[
\cos(\theta) = \frac{A \cdot B}{||A|| ||B||}
\]

![alt text](image-5.png)
where:
- ** A = Image Embedding**
- ** B = Text Embedding**
- ** A.B** = Dot Product
- ** ||A||,||B||** = Magnitudes (Vector Norms)

---

## **3Ô∏è‚É£ Compute Dot Product**

### **üîπ Dot Product of Image üê± and "A cat"**
\[
(0.2 x 0.2) + (0.7 x 0.7) + (0.1 x 0.1) + (0.9 x 0.9)
\]
\[
= 0.04 + 0.49 + 0.01 + 0.81 = 1.35
\]

### **üîπ Dot Product of Image üê± and "A dog"**
\[
(0.2 x 0.3) + (0.7 x 0.6) + (0.1 x 0.2) + (0.9 x 0.8)
\]
\[
= 0.06 + 0.42 + 0.02 + 0.72 = 1.22
\]

### **üîπ Dot Product of Image üê± and "A car"**
\[
(0.2 x 0.9) + (0.7 x 0.1) + (0.1 x 0.4) + (0.9 x 0.2)
\]
\[
= 0.18 + 0.07 + 0.04 + 0.18 = 0.47
\]

---

## **4Ô∏è‚É£ Compute Magnitudes (Vector Norms)**

### **üîπ Magnitude of Image üê±**

![alt text](image-6.png)

---

## **5Ô∏è‚É£ Compute Cosine Similarity**

### **üîπ Cosine Similarity with "A cat"**

![alt text](image-7.png)

---

## **üìä 6Ô∏è‚É£ Final Ranking Based on Cosine Similarity**
| **Text Option** | **Cosine Similarity** | **Ranking** |
|---------------|------------------|-------------|
| **"A cat"** | **1.00** | ‚úÖ **Best Match** |
| **"A dog"** | **0.99** | üîπ **Second Best** |
| **"A car"** | **0.40** | ‚ùå **Least Similar** |

üéØ Since **"A cat"** has the highest similarity score (**1.00**), it is the best match for the image of the cat! üê±

---

### **üìå Summary**
‚úÖ Cosine Similarity ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Ñ‡∏•‡πâ‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° ‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û 
‚úÖ ‡∏Ñ‡πà‡∏≤‡∏¢‡∏¥‡πà‡∏á‡∏™‡∏π‡∏á ‡∏¢‡∏¥‡πà‡∏á‡πÄ‡∏´‡∏°‡∏∑‡∏≠‡∏ô
‚úÖ ‡πÉ‡∏ä‡πâ‡πÉ‡∏ô CLIP, Vision-Language Models 

üîπ ‡∏™‡∏£‡∏∏‡∏õ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á CLIP ‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÑ‡∏î‡πâ‡∏£‡∏±‡∏ö "‡∏â‡∏±‡∏ô‡∏£‡∏±‡∏Å‡πÄ‡∏ò‡∏≠" <br>
1Ô∏è‚É£ Tokenization ‚Üí ‡πÅ‡∏õ‡∏•‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏õ‡πá‡∏ô Token IDs <br>
2Ô∏è‚É£ Text Encoding ‚Üí ‡∏™‡∏£‡πâ‡∏≤‡∏á Text Embedding ‡∏î‡πâ‡∏ß‡∏¢ Transformer <br>
3Ô∏è‚É£ Image Matching (‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏†‡∏≤‡∏û) ‚Üí ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö Text vs. Image Embedding <br>
4Ô∏è‚É£ Similarity Computation ‚Üí ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì Cosine Similarity ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û <br>

‚úÖ ‡∏ñ‡πâ‡∏≤‡∏°‡∏µ‡∏†‡∏≤‡∏û‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö ‚Üí ‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏£‡∏π‡∏õ‡∏ó‡∏µ‡πà‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Ç‡πâ‡∏≠‡∏á <br>
‚úÖ ‡∏ñ‡πâ‡∏≤‡πÑ‡∏°‡πà‡∏°‡∏µ‡∏†‡∏≤‡∏û ‚Üí ‡πÉ‡∏ä‡πâ embedding ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏ö‡∏£‡∏¥‡∏ö‡∏ó <br>


üñºÔ∏è Step-by-Step Process of a Vision Transformer (ViT)
‡πÄ‡∏°‡∏∑‡πà‡∏≠‡πÉ‡∏™‡πà‡∏†‡∏≤‡∏û‡πÄ‡∏Ç‡πâ‡∏≤‡πÑ‡∏õ ViT ‡∏à‡∏∞‡∏ó‡∏≥‡∏î‡∏±‡∏á‡∏ï‡πà‡∏≠‡πÑ‡∏õ‡∏ô‡∏µ‡πâ:

1Ô∏è‚É£ ‡πÅ‡∏ö‡πà‡∏á‡∏£‡∏π‡∏õ‡πÉ‡∏´‡∏ç‡πà ‡πÜ ‡πÉ‡∏´‡πâ‡πÄ‡∏õ‡πá‡∏ô‡∏£‡∏π‡∏õ‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ (‡πÄ‡∏ä‡πà‡∏ô, 16x16 pixels).
‡πÅ‡∏•‡∏∞‡∏£‡∏π‡∏õ‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ ‡∏à‡∏∞‡∏ñ‡∏π‡∏Å‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÄ‡∏õ‡πá‡∏ô ‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç flat vector

üîπ Example: <br>
‡πÄ‡∏ä‡πà‡∏ô ‡∏£‡∏π‡∏õ‡∏Ç‡∏ô‡∏≤‡∏î 256√ó256 pixels ‡∏ñ‡∏π‡∏Å‡πÅ‡∏ö‡πà‡∏á‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ ‡πÄ‡∏õ‡πá‡∏ô‡∏Ç‡∏ô‡∏≤‡∏î 16√ó16 patches, ‡πÑ‡∏î‡πâ‡∏à‡∏≤‡∏Å 256 / 16 = 16 patches.

2Ô∏è‚É£ ‡∏™‡∏£‡πâ‡∏≤‡∏á Patch Embeddings <br>
‡∏£‡∏π‡∏õ‡∏¢‡πà‡∏≠‡∏¢ ‡πÜ ‡∏à‡∏∞‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡πà‡∏≤‡πÑ‡∏õ‡πÄ‡∏õ‡πá‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡∏Ç <br>
This embedding represents key features of the patch. <br>
üîπ Formula for Patch Embeddings:

![formular patch embedding](image-1.png)

3Ô∏è‚É£ ‡πÉ‡∏™‡πà‡∏Ñ‡πà‡∏≤ Token <br>
A [CLS] token ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡∏†‡∏≤‡∏¢‡∏´‡∏•‡∏±‡∏á <br>

4Ô∏è‚É£ ‡πÉ‡∏ä‡πâ Transformer Encoder (Self-Attention Mechanism)
‡πÉ‡∏ä‡πâ‡∏Å‡∏£‡∏∞‡∏ö‡∏ß‡∏ô‡∏Å‡∏≤‡∏£ patch embeddings using multi-head self-attention (MHSA). ‡πÅ‡∏ï‡πà‡∏•‡∏∞ patch ‡∏à‡∏∞‡πÄ‡∏Ç‡πâ‡∏≤‡∏£‡πà‡∏ß‡∏° ‡∏Å‡∏±‡∏ö patch ‡∏≠‡∏∑‡πà‡∏ô ‡πÜ ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏´‡∏≤‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå

![self-attention mechanism](image-2.png)

5Ô∏è‚É£ ‡πÄ‡∏à‡∏ô output <br>


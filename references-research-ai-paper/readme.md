### ‡∏´‡πâ‡∏≠‡∏á‡∏™‡∏°‡∏∏‡∏î‡∏™‡πà‡∏ß‡∏ô‡∏ï‡∏±‡∏ß‡πÄ‡∏•‡πá‡∏Å ‡πÜ ‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡∏î‡πâ‡∏≤‡∏ô AI ‡∏ó‡∏µ‡πà‡∏ô‡πà‡∏≤‡∏™‡∏ô‡πÉ‡∏à

## üîóReferences for Text to image generative AI

Image generative Areana! https://huggingface.co/spaces/ArtificialAnalysis/Text-to-Image-Leaderboard

| ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠    | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏¢‡πà‡∏≠‡πÜ |  Link |
| -------- | ------- | ------- |
| High-Resolution Image Synthesis with Latent Diffusion Models  |  ‡∏ï‡πâ‡∏ô‡∏Å‡∏≥‡πÄ‡∏ô‡∏¥‡∏î Stable diffusion    |https://arxiv.org/abs/2112.10752|
| Reproducible scaling laws for contrastive language-image learning | ‡∏á‡∏≤‡∏ô‡∏ß‡∏¥‡∏à‡∏±‡∏¢‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö CLIP ‡∏Ç‡∏≠‡∏á OpenAI     |https://arxiv.org/abs/2212.07143 <br> ‡∏•‡∏¥‡πâ‡∏á‡πÄ‡∏û‡∏¥‡πà‡∏°‡πÄ‡∏ï‡∏¥‡∏° https://github.com/mlfoundations/open_clip|
| Adding Conditional Control to Text-to-Image Diffusion Models    | ‡∏Å‡∏≤‡∏£‡∏ó‡∏≥‡∏á‡∏≤‡∏ô Control net    |[TBA](https://arxiv.org/pdf/2302.05543)|

## üîóReferences for Sound/Voice/Musics generative AI 

Sound/Voice clone areana ! https://huggingface.co/spaces/TTS-AGI/TTS-Arena

| ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠    | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏¢‡πà‡∏≠‡πÜ |  Link |
| -------- | ------- | ------- |
| PyThaiNLP: Thai Natural Language Processing in Python   | TBA    |https://arxiv.org/pdf/2312.04649|
| PyThaiNLP open source   | TBA    |https://pythainlp.org/thai-tutorials/index.html|
| StyleTTS 2: Towards Human-Level Text-to-Speech through Style Diffusion  | TBA    |https://arxiv.org/abs/2306.07691|

## üîóReference for Video generative AI

The areana!!  https://huggingface.co/spaces/ArtificialAnalysis/Text-to-Image-Leaderboard

| ‡∏´‡∏±‡∏ß‡∏Ç‡πâ‡∏≠    | ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏¢‡πà‡∏≠‡πÜ |  Link |
| -------- | ------- | ------- |
| TBA    | TBA    |TBA|


## üîó Summary: Research Papers on Image-to-Text Models

| **Topic** | **Paper Title** | **Authors** | **Year** | **Link** | **Summary (Thai)** |
|-----------|----------------|-------------|----------|----------|----------------------|
| **Vision Transformer (ViT)** | An Image is Worth 16x16 Words | Dosovitskiy et al. | 2020 | [arXiv](https://arxiv.org/abs/2010.11929) | ‡πÄ‡∏™‡∏ô‡∏≠‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ Transformer ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏à‡∏≥‡πÅ‡∏ô‡∏Å‡∏£‡∏π‡∏õ‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡πÑ‡∏°‡πà‡πÉ‡∏ä‡πâ Convolutional Layers |
| **Convolutional Neural Networks (CNNs)** | Gradient-Based Learning Applied to Document Recognition | LeCun et al. | 1998 | [Paper](http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf) | ‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏Ç‡∏≠‡∏á CNN ‡πÅ‡∏•‡∏∞‡∏Å‡∏≤‡∏£‡πÉ‡∏ä‡πâ‡πÉ‡∏ô‡∏á‡∏≤‡∏ô‡∏à‡∏î‡∏à‡∏≥‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ |
| **Recurrent Neural Networks (RNNs)** | Long Short-Term Memory | Hochreiter & Schmidhuber | 1997 | [Paper](https://www.bioinf.jku.at/publications/older/2604.pdf) | ‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ LSTM ‡∏ã‡∏∂‡πà‡∏á‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ RNN ‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏î‡∏Å‡∏≤‡∏£‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏•‡∏≥‡∏î‡∏±‡∏ö‡∏¢‡∏≤‡∏ß‡πÑ‡∏î‡πâ‡∏î‡∏µ‡∏Ç‡∏∂‡πâ‡∏ô |
| **CLIP** | Learning Transferable Visual Models From Natural Language Supervision | Radford et al. | 2021 | [arXiv](https://arxiv.org/abs/2103.00020) | ‡∏û‡∏±‡∏í‡∏ô‡∏≤‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Contrastive Learning |
| **BLIP** | Bootstrapped Language-Image Pretraining | Li et al. | 2022 | [arXiv](https://arxiv.org/abs/2201.12086) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏£‡∏∞‡∏´‡∏ß‡πà‡∏≤‡∏á‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û |
| **GPT-4 Vision (GPT-4V)** | GPT-4 Technical Report | OpenAI | 2023 | [Paper](https://cdn.openai.com/papers/gpt-4.pdf) | ‡∏£‡∏≤‡∏¢‡∏á‡∏≤‡∏ô‡∏ó‡∏≤‡∏á‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏Ç‡∏≠‡∏á GPT-4 ‡πÅ‡∏•‡∏∞‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏î‡πâ‡∏≤‡∏ô‡∏Å‡∏≤‡∏£‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏û |
| **SimVLM** | Simple Visual Language Model Pretraining | Wang et al. | 2021 | [arXiv](https://arxiv.org/abs/2108.10904) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏†‡∏≤‡∏©‡∏≤‡∏Å‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏°‡∏≠‡∏á‡πÄ‡∏´‡πá‡∏ô ‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡πÅ‡∏ö‡∏ö‡∏≠‡πà‡∏≠‡∏ô (weakly supervised) |
| **LLaVA** | Large Language and Vision Assistant | Liu et al. | 2023 | [arXiv](https://arxiv.org/abs/2304.08485) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏° LLM ‡∏Å‡∏±‡∏ö‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏∞‡∏°‡∏ß‡∏•‡∏ú‡∏•‡∏†‡∏≤‡∏û‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û |
| **Flamingo** | A Visual Language Model for Few-Shot Learning | Alayrac et al. | 2022 | [arXiv](https://arxiv.org/abs/2204.14198) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏ï‡∏±‡∏ß‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏ô‡πâ‡∏≠‡∏¢‡πÅ‡∏•‡∏∞‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ó‡∏≥‡∏á‡∏≤‡∏ô‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ö‡∏ó‡∏±‡πâ‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏û |
| **Kosmos-2** | Grounding Multimodal Large Language Models | Huang et al. | 2023 | [arXiv](https://arxiv.org/abs/2306.14824) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÇ‡∏î‡∏¢‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ |
| **GIT** | Generative Image-to-Text Transformer | Wang et al. | 2022 | [arXiv](https://arxiv.org/abs/2205.14100) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏ö‡∏£‡∏£‡∏¢‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ Transformer |
| **Show and Tell** | A Neural Image Caption Generator | Vinyals et al. | 2015 | [arXiv](https://arxiv.org/abs/1411.4555) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏£‡∏Å ‡πÜ ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ CNN + LSTM ‡πÉ‡∏ô‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| **OCR (Optical Character Recognition)** | What is Wrong with Scene Text Recognition Model Comparisons? | Baek et al. | 2019 | [arXiv](https://arxiv.org/abs/1904.01906) | ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏õ‡∏±‡∏ç‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡πÅ‡∏ô‡∏ß‡∏ó‡∏≤‡∏á‡∏Å‡∏≤‡∏£‡πÄ‡∏õ‡∏£‡∏µ‡∏¢‡∏ö‡πÄ‡∏ó‡∏µ‡∏¢‡∏ö‡πÇ‡∏°‡πÄ‡∏î‡∏• OCR ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÉ‡∏ô‡∏†‡∏≤‡∏û |
| **Word2Vec** | Efficient Estimation of Word Representations in Vector Space | Mikolov et al. | 2013 | [arXiv](https://arxiv.org/abs/1301.3781) | ‡∏ô‡∏≥‡πÄ‡∏™‡∏ô‡∏≠ Word2Vec ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏Ñ‡∏≥‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡∏™‡∏±‡∏°‡∏û‡∏±‡∏ô‡∏ò‡πå‡πÄ‡∏ä‡∏¥‡∏á‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢ |
| **GloVe** | Global Vectors for Word Representation | Pennington et al. | 2014 | [Stanford NLP](https://nlp.stanford.edu/pubs/glove.pdf) | ‡πÄ‡∏ó‡∏Ñ‡∏ô‡∏¥‡∏Ñ‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ‡∏Ñ‡∏≥‡∏ô‡∏ß‡∏ì‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡πÇ‡∏î‡∏¢‡∏≠‡πâ‡∏≤‡∏á‡∏≠‡∏¥‡∏á‡∏à‡∏≤‡∏Å‡∏™‡∏ñ‡∏¥‡∏ï‡∏¥‡∏Å‡∏≤‡∏£‡∏õ‡∏£‡∏≤‡∏Å‡∏è‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥ |
| **BERT** | Pre-training of Deep Bidirectional Transformers | Devlin et al. | 2019 | [arXiv](https://arxiv.org/abs/1810.04805) | ‡πÇ‡∏°‡πÄ‡∏î‡∏• Transformer ‡∏ó‡∏µ‡πà‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡∏ö‡∏£‡∏¥‡∏ö‡∏ó‡∏Ç‡∏≠‡∏á‡∏Ñ‡∏≥‡∏à‡∏≤‡∏Å‡∏ó‡∏±‡πâ‡∏á‡∏™‡∏≠‡∏á‡∏ó‡∏¥‡∏®‡∏ó‡∏≤‡∏á‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏ä‡πâ‡πÉ‡∏ô NLP |
| **Universal Sentence Encoder (USE)** | Universal Sentence Encoder | Cer et al. | 2018 | [arXiv](https://arxiv.org/abs/1803.11175) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏ä‡πà‡∏ß‡∏¢‡πÉ‡∏´‡πâ‡∏Å‡∏≤‡∏£‡πÅ‡∏õ‡∏•‡∏á‡∏õ‡∏£‡∏∞‡πÇ‡∏¢‡∏Ñ‡πÄ‡∏õ‡πá‡∏ô‡πÄ‡∏ß‡∏Å‡πÄ‡∏ï‡∏≠‡∏£‡πå‡∏™‡∏≤‡∏°‡∏≤‡∏£‡∏ñ‡∏ô‡∏≥‡πÑ‡∏õ‡πÉ‡∏ä‡πâ‡∏Å‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ï‡πà‡∏≤‡∏á ‡πÜ ‡πÑ‡∏î‡πâ‡∏≠‡∏¢‡πà‡∏≤‡∏á‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û |
| **OpenAI Embeddings** | OpenAI Embeddings: Text Representations for Semantic Search | OpenAI | 2022 | [Docs](https://beta.openai.com/docs/guides/embeddings) | ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡πÉ‡∏´‡πâ embedding ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡πÅ‡∏•‡∏∞‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏Ñ‡∏ß‡∏≤‡∏°‡∏´‡∏°‡∏≤‡∏¢‡∏Ç‡∏≠‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏° |
| **Florence** | [Florence: A New Foundation Model for Computer Vision](https://arxiv.org/abs/2111.11432) | [arXiv:2111.11432](https://arxiv.org/pdf/2111.11432) | 2021 || ‡πÄ‡∏ô‡πâ‡∏ô‡πÄ‡∏£‡∏∑‡πà‡∏≠‡∏á compute vision |
| **CLIP** | [Multimodal Foundation Models](https://www.nowpublishers.com/article/Details/CGV-110) | [Now Publishers](https://www.nowpublishers.com/article/Details/CGV-110) | 2024 || ‡∏à‡∏±‡∏ö‡∏Ñ‡∏π‡πà‡∏†‡∏≤‡∏û‡∏Å‡∏±‡∏ö‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏•‡∏∞‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Ñ‡πâ‡∏ô‡∏´‡∏≤‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û |
| **BLIP** | [Benchmark Evaluations of Large Vision-Language Models](https://arxiv.org/abs/2501.02189) | [arXiv 2501.02189](https://arxiv.org/abs/2501.02189) | 2025 || ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û (Image Captioning) ‡πÅ‡∏•‡∏∞‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û |
| **GPT-4 Vision** | [Unified Approaches for Vision-Language](https://search.proquest.com/openview/9b0e3d8d3074e04922702f883cc9f7c0/1) | [ProQuest](https://search.proquest.com/openview/9b0e3d8d3074e04922702f883cc9f7c0/1) | 2024 || ‡∏ß‡∏¥‡πÄ‡∏Ñ‡∏£‡∏≤‡∏∞‡∏´‡πå‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏ï‡∏≠‡∏ö‡πÄ‡∏ä‡∏¥‡∏á‡∏ï‡∏£‡∏£‡∏Å‡∏∞‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏†‡∏≤‡∏û |
| **SimVLM** | [Simple Visual Language Model](https://arxiv.org/abs/2111.09883) | [arXiv 2111.09883](https://arxiv.org/abs/2111.09883) | 2021 || ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡∏ù‡∏∂‡∏Å‡∏ù‡∏ô‡πÅ‡∏ö‡∏ö‡∏≠‡πà‡∏≠‡∏ô (weakly supervised) ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡πÉ‡∏´‡πâ‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏õ‡∏£‡∏∞‡∏™‡∏¥‡∏ó‡∏ò‡∏¥‡∏†‡∏≤‡∏û |
| **LLaVA** | [Multimodal Large Language Models](https://aclanthology.org/2024.ccl-2.1/) | [ACL Anthology](https://aclanthology.org/2024.ccl-2.1/) | 2024 || ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏ó‡∏µ‡πà‡∏£‡∏ß‡∏°‡∏†‡∏≤‡∏©‡∏≤‡πÅ‡∏•‡∏∞‡∏†‡∏≤‡∏û ‡∏ä‡πà‡∏ß‡∏¢‡∏ï‡∏≠‡∏ö‡∏Ñ‡∏≥‡∏ñ‡∏≤‡∏°‡πÄ‡∏Å‡∏µ‡πà‡∏¢‡∏ß‡∏Å‡∏±‡∏ö‡∏†‡∏≤‡∏û‡πÇ‡∏î‡∏¢‡πÉ‡∏ä‡πâ LLM |
| **Flamingo** | [Few-Shot Learning for Vision-Language](https://arxiv.org/abs/2204.14198) | [DeepMind](https://arxiv.org/abs/2204.14198) | 2022 || ‡πÉ‡∏ä‡πâ‡∏Å‡∏≤‡∏£‡πÄ‡∏£‡∏µ‡∏¢‡∏ô‡∏£‡∏π‡πâ‡πÅ‡∏ö‡∏ö few-shot ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏á‡∏≤‡∏ô‡∏ó‡∏µ‡πà‡∏ï‡πâ‡∏≠‡∏á‡∏Å‡∏≤‡∏£‡πÉ‡∏´‡πâ‡πÄ‡∏Ç‡πâ‡∏≤‡πÉ‡∏à‡∏†‡∏≤‡∏û‡πÅ‡∏•‡∏∞‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏£‡πà‡∏ß‡∏°‡∏Å‡∏±‡∏ô |
| **Kosmos-2** | [Grounded Multimodal Generation](https://arxiv.org/abs/2306.14824) | [Microsoft](https://arxiv.org/abs/2306.14824) | 2023 || ‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏´‡∏£‡∏∑‡∏≠‡∏ß‡∏¥‡∏î‡∏µ‡πÇ‡∏≠ ‡πÉ‡∏ä‡πâ‡∏™‡∏≥‡∏´‡∏£‡∏±‡∏ö‡∏Å‡∏≤‡∏£‡∏™‡∏£‡πâ‡∏≤‡∏á‡πÄ‡∏ô‡∏∑‡πâ‡∏≠‡∏´‡∏≤‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏û‡∏∑‡πâ‡∏ô‡∏ê‡∏≤‡∏ô‡∏à‡∏≤‡∏Å‡∏Ç‡πâ‡∏≠‡∏°‡∏π‡∏•‡∏†‡∏≤‡∏û |
| **GIT** | [Generative Image-to-Text Transformer](https://arxiv.org/abs/2205.14100) | [arXiv 2205.14100](https://arxiv.org/abs/2205.14100) | 2022 || ‡πÇ‡∏°‡πÄ‡∏î‡∏•‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û‡∏ó‡∏µ‡πà‡∏°‡∏µ‡∏Ñ‡∏ß‡∏≤‡∏°‡πÅ‡∏°‡πà‡∏ô‡∏¢‡∏≥‡∏™‡∏π‡∏á |
| **Show and Tell** | [A Neural Image Caption Generator](https://arxiv.org/abs/1411.4555) | [arXiv 1411.4555](https://arxiv.org/abs/1411.4555) | 2015 || ‡∏´‡∏ô‡∏∂‡πà‡∏á‡πÉ‡∏ô‡πÇ‡∏°‡πÄ‡∏î‡∏•‡πÅ‡∏£‡∏Å‡∏ó‡∏µ‡πà‡πÉ‡∏ä‡πâ CNN+LSTM ‡πÄ‡∏û‡∏∑‡πà‡∏≠‡∏™‡∏£‡πâ‡∏≤‡∏á‡∏Ñ‡∏≥‡∏≠‡∏ò‡∏¥‡∏ö‡∏≤‡∏¢‡∏†‡∏≤‡∏û‡∏≠‡∏±‡∏ï‡πÇ‡∏ô‡∏°‡∏±‡∏ï‡∏¥ |
| **OCR (Tesseract, TrOCR)** | [OCR for Vision-Language Tasks](https://arxiv.org/abs/2401.02276) | [arXiv 2401.02276](https://arxiv.org/abs/2401.02276) | 2024 || ‡∏ï‡∏£‡∏ß‡∏à‡∏à‡∏±‡∏ö‡πÅ‡∏•‡∏∞‡∏î‡∏∂‡∏á‡∏Ç‡πâ‡∏≠‡∏Ñ‡∏ß‡∏≤‡∏°‡∏à‡∏≤‡∏Å‡∏†‡∏≤‡∏û (OCR) ‡πÄ‡∏ä‡πà‡∏ô ‡πÄ‡∏≠‡∏Å‡∏™‡∏≤‡∏£ ‡∏™‡∏±‡∏ç‡∏•‡∏±‡∏Å‡∏©‡∏ì‡πå ‡πÅ‡∏•‡∏∞‡∏õ‡πâ‡∏≤‡∏¢‡∏ñ‡∏ô‡∏ô |

